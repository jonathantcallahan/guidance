{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonathantcallahan/guidance/blob/main/book_processing_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFy5tIxFFQgw",
        "outputId": "6e35ade9-e92a-474c-d2e5-56c9b12d07e2"
      },
      "outputs": [],
      "source": [
        "%pip install openai\n",
        "%pip install chardet\n",
        "%pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WT7J6KmGcx1d"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import uuid\n",
        "import json\n",
        "from ftfy import fix_encoding\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chardet\n",
        "\n",
        "def detect_encoding(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        raw_data = file.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "    encoding = result['encoding']\n",
        "    return encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filenames = []\n",
        "\n",
        "for filename in os.listdir('books'):\n",
        "    filenames.append(filename)\n",
        "\n",
        "print(f'Books in list: {len(filenames)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#generate quetsions to the \"answers\" extracted from the text\n",
        "def answer_gpt(answer):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-2024-05-13\",\n",
        "        messages=[\n",
        "            { \n",
        "                \"role\": \"system\", \n",
        "                \"content\": \"You are assisting in the generation of training data for fine-tuning. You will receive a chunk of text, and will respond with a short casually phrased question to which the chunk of text you received would be an expected answer. The person who generated the answer is Alan Watt's and often he will give a response that answers a question only indirectly. The question should not exactly contain the subject matter of the answer. The question you create should be one to which the answer would be a correct indirect or metaphorical answer.\" \n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\", \n",
        "                \"content\": f\"Return a short casually phrased question to which this text would be an appropriate. The question should not reference the core subject matter of the answer in an overt way. : {answer}\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#split chunks into sections that could reasonably be the answer to a question\n",
        "def chunk_gpt(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            { \n",
        "                \"role\": \"system\", \n",
        "                \"content\": \"You are a document processor used to create fine-tuning data. You will receive 5000 characters worth of a book, and extract portions of text that would be coherent as the answer to a theoretical, unspecified question. Each answer can be up to 200 words. The cohesive answers within the text may directly following each other and there may be space between them that needs to be removed. The response you provide should strictly be the series of cohesive thoughts identified within the content separated by line breaks. Minor grammatical may be made as needed. \"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\", \n",
        "                \"content\": f\"Extract chunks of text from this page that would be coherent as responses to an unspecified question. :\\n\\n{text}\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip().split('\\n') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#loop through all of the processed \"answers\" and generate questions\n",
        "def process_questions(processed_answers, book_name):\n",
        "    for i in range(len(processed_answers)):\n",
        "        \n",
        "        #limiting cycles for testing\n",
        "        if i > 2 and debugger == True:\n",
        "            continue\n",
        "\n",
        "        answer = processed_answers[i]\n",
        "        #remove blanks\n",
        "        if len(answer) < 30:\n",
        "            continue\n",
        "\n",
        "        #question = answer_gpt(answer)\n",
        "\n",
        "        json_obj = { \n",
        "            \"book\" : book_name,\n",
        "            \"instruction\" : \"You are English author and intellectual Alan Watts. Please answer the following question using your standard speech patterns but do not over-embellish.\", \n",
        "            \"input\" : \"\", \n",
        "            \"output\" : answer \n",
        "        }\n",
        "        \n",
        "        processed_json.append(json_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#loop through the chunks of a book\n",
        "def process_book_chunks(text_chunks, book_name):\n",
        "    print(f'Processing {len(text_chunks)} chunks for {book_name}')\n",
        "    for i in range(len(text_chunks)):\n",
        "        chunk = fix_encoding(text_chunks[i])\n",
        "        \n",
        "        #skip the first and last pages which are usually credits and other misellaneous content\n",
        "        if i > len(text_chunks)-8 or i < 8:\n",
        "            continue\n",
        "\n",
        "        #limiting requests for testing purposes\n",
        "        if i > 9 and debugger == True:\n",
        "            continue\n",
        "\n",
        "        chunk_entry(chunk, book_name)\n",
        "        #processed_answers = chunk_gpt(chunk)\n",
        "        #process_questions(processed_answers, book_name)\n",
        "    print(f'Completed processing {book_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#process books into chunks of characters\n",
        "def process_books():\n",
        "    chunk_size = 5000 \n",
        "    for i in range(len(filenames)):\n",
        "        \n",
        "        #limiting cycles for testing\n",
        "        if i > 0 and debugger == True:\n",
        "            continue\n",
        "        \n",
        "        encoding = detect_encoding(f'books/{filenames[i]}')\n",
        "        with open(f'books/{filenames[i]}', 'r', encoding=encoding, errors='replace') as file:\n",
        "            content = file.read().replace('\\n','')\n",
        "            text_chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
        "\n",
        "            process_book_chunks(text_chunks, filenames[i])\n",
        "            \n",
        "\n",
        "    #print(fix_encoding(json.dumps(processed_json, indent=4, ensure_ascii=False)))\n",
        "    print(fix_encoding(json.dumps(chunk_list, indent=4, ensure_ascii=False)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_list = []\n",
        "def chunk_entry(chunk, book):\n",
        "    bulk_entry = { \"custom_id\": f\"{book}-{str(uuid.uuid4())}\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": { \"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a document processor used to create fine-tuning data. You will receive 5000 characters worth of a book, and extract portions of text that would be coherent as the answer to a theoretical, unspecified question. Each answer can be up to 200 words. The cohesive answers within the text may directly following each other and there may be space between them that needs to be removed. The response you provide should strictly be the series of cohesive thoughts identified within the content separated by line breaks. Minor grammatical may be made as needed.\"},{\"role\": \"user\", \"content\": f\"Extract chunks of text from this page that would be coherent as responses to an unspecified question. :\\n\\n{chunk}\"}],\"max_tokens\": 5000}}\n",
        "    chunk_list.append(bulk_entry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_json = [] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = 'batch_answer_upload.jsonl'\n",
        "\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    for entry in chunk_list:\n",
        "        file.write(json.dumps(entry, ensure_ascii=False) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_input_file = client.files.create(\n",
        "  file=open(\"batch_answer_upload.jsonl\", \"rb\"),\n",
        "  purpose=\"batch\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_input_file_id = batch_input_file.id\n",
        "\n",
        "client.batches.create(\n",
        "    input_file_id=batch_input_file_id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "      \"description\": \"processing chunks into answers\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.batches.retrieve(\"batch_eoJiEIRMqNH9P5oKCu7LO6M9\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "debugger = False\n",
        "process_books()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('chunks_3_through_189','r') as f1:\n",
        "    book_one = json.load(f1)\n",
        "\n",
        "with open('more_chunks', 'r') as f2:\n",
        "    book_two = json.load(f2)\n",
        "\n",
        "combined_data = book_one + book_two\n",
        "\n",
        "with open('first_batch_total', 'w') as f_combined:\n",
        "    json.dump(combined_data, f_combined, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(fix_encoding(json.dumps(processed_json, indent=4, ensure_ascii=False)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyORdGX/o0kYsZlGIyCOuat2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
